{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open the new dataset\n",
    "\n",
    "import codecs, nltk\n",
    "\n",
    "article = codecs.open(\"../datasets/CleanedArticles/15.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "# take one single sentence \n",
    "\n",
    "sentence = sentences[1]\n",
    "\n",
    "#tokenize it\n",
    "\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
    "pos_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "print (pos_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining lemmatization and pos tagging\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = []\n",
    "\n",
    "for word,pos in pos_sentence:\n",
    "    \n",
    "    # if it's a verb - then we tell that to the lemmatizer\n",
    "    if pos[0] == \"V\":\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word,\"v\")\n",
    "    else:\n",
    "    # otherwise, work as usual\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "    # we append the results\n",
    "    lemma_words.append(lemma)\n",
    "    \n",
    "print (lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's now define a function that does all we need\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_sentence = nlp_pipeline(sentence)\n",
    "print (clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's take an entire article and use our pipeline!\n",
    "\n",
    "clean_article = nlp_pipeline(article)\n",
    "print (clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# let's isolate each word - you do this using a set (another type of object in python)\n",
    "\n",
    "unique_words = \n",
    "\n",
    "# let's check how many senses each word has\n",
    "for word in unique_words:\n",
    "    print (word, len(wn.synsets(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = \"loan\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    print (\"\\ndefinition\")\n",
    "    print(sense.definition())\n",
    "    \n",
    "    # get a textual example\n",
    "    print (\"\\nexample\")\n",
    "    print(sense.examples())\n",
    "    \n",
    "    # get hypernymy\n",
    "    print (\"\\nhypernymy\")\n",
    "    print(sense.hypernyms())\n",
    "\n",
    "    # get hyponyms\n",
    "    print (\"\\nhyponyms\")\n",
    "    print(sense.hyponyms())\n",
    "        \n",
    "    # this is a way of getting synonyms - there are others\n",
    "    print (\"\\nsynonyms\")\n",
    "    print (sense.lemma_names())\n",
    "    \n",
    "    # this is for getting antonyms - works especially with adjectives \n",
    "    print (\"\\nantonyms\")\n",
    "    print (sense.lemmas()[0].antonyms())\n",
    "    \n",
    "    print (\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1:\", clean_sent1)\n",
    "print (\"clean sent 2:\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "# for each possible sense of \"cell\" you can, for instance, check the overlap between the definition and the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework: find the best sense - implement your version of the Lesk algorithm: https://en.wikipedia.org/wiki/Lesk_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "homework 2: get the json file with the tweets from Donald Trump and improve his vocabulary by changing his poor choice of adjectives with more sophisticated synonyms (e.g. \"bad ratings on the Emmys last night\" -> \"substandard ratings on the Emmys last night\") \n",
    " \n",
    "or\n",
    "\n",
    "make his tweets nicer by changing adjectives with related antonyms (e.g. \"bad ratings on the Emmys last night\" -> \"excellent ratings on the Emmys last night\") \n",
    "\n",
    "to do you need to combine:\n",
    "\n",
    "- text processing (POS tagging + WordNet)\n",
    "- and to find a solution for knowing if a word is \"more sophisticated\" than another one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
